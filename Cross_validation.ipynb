{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc83a90-c899-484e-87c8-a0ac9296df7a",
   "metadata": {},
   "source": [
    "# Cross Validation (CV)\n",
    "- # GEORGE JEFF ONYANGO\n",
    "Cross-validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves splitting the dataset into multiple subsets, training the model on some subsets, and testing it on others, repeating this process multiple times to obtain a more accurate estimate of the model's performance. This method helps prevent overfitting, ensuring that the model can generalize well to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5660b-a084-4190-a1ab-b2fd9c0a04a5",
   "metadata": {},
   "source": [
    "One common type of cross-validation is k-fold cross-validation, where the dataset is divided into k subsets (folds). The model is trained on k-1 folds and tested on the remaining fold, and this process is repeated k times, with each fold serving as the test set once. The results from each iteration are then averaged to provide a final performance estimate.\n",
    "\n",
    "As we will be trying to classify different species of iris flowers we will need to import a classifier model, for this exercise we will be using a DecisionTreeClassifier. We will also need to import CV modules from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d4ca61-8a65-4c9f-a7d8-781b413f884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7802ed2c-d224-4524-9529-bb7d0dba0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load datasets'''\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a297968-13ee-4ce3-b4d5-ffd1e4a2c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create and fit the model for evaluation'''\n",
    "clf = DecisionTreeClassifier(random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c949b3d-1468-419c-8ca9-f66d546ade9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 1.        , 0.83333333, 0.93333333, 0.8       ]),\n",
       " <function ndarray.mean>,\n",
       " 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''evaluate the model and see how it performs on each k-fold'''\n",
    "k_folds = KFold(n_splits=5)\n",
    "scores = cross_val_score(clf,X,y,cv=k_folds)\n",
    "\n",
    "scores, scores.mean, len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f3a90-4b6d-4409-b7a3-1e86eeebd258",
   "metadata": {},
   "source": [
    "# Stratified K-Fold\n",
    "In cases where classes are imbalanced we need a way to account for the imbalance in both the train and validation sets. To do so we can stratify the target classes, meaning that both sets will have an equal proportion of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e46dab6-9696-4ac1-a1a5-ecca0e0aca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
      "Average CV Score:  0.9533333333333334\n",
      "Number of CV Scores used in Average:  5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "sk_folds = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = sk_folds)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c5982-ab5e-4552-83ed-11d72d1003bd",
   "metadata": {},
   "source": [
    "While the number of folds is the same, the average CV increases from the basic k-fold when making sure there is stratified classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aadcd1-67e8-440c-9d19-7c01d87ee867",
   "metadata": {},
   "source": [
    "# Leave-One-Out (LOO)\n",
    "Instead of selecting the number of splits in the training data set like k-fold LeaveOneOut, utilize 1 observation to validate and n-1 observations to train. This method is an exaustive technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b968211-b215-4296-9926-4b305150ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Average CV Score:  0.94\n",
      "Number of CV Scores used in Average:  150\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = loo)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c72f65-6a41-4bc4-815d-e4adbacf4d99",
   "metadata": {},
   "source": [
    "We can observe that the number of cross validation scores performed is equal to the number of observations in the dataset. In this case there are 150 observations in the iris dataset.\n",
    "\n",
    "The average CV score is 94%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6e0cb-7e45-42b3-8342-488f34e0650f",
   "metadata": {},
   "source": [
    "# Leave-P-Out (LPO)\n",
    "Leave-P-Out is simply a nuanced diffence to the Leave-One-Out idea, in that we can select the number of p to use in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8c963c7-6272-4e30-af60-58c4461dcfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [1. 1. 1. ... 1. 1. 1.]\n",
      "Average CV Score:  0.9382997762863534\n",
      "Number of CV Scores used in Average:  11175\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeavePOut, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = lpo)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c32e7-8ca3-496f-b6bf-9c3e2a5cb62c",
   "metadata": {},
   "source": [
    "This is an exhaustive method we many more scores being calculated than Leave-One-Out, even with a p = 2, yet it achieves roughly the same average CV score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d50dfc-b0ce-4b69-a012-9f8665a6efb7",
   "metadata": {},
   "source": [
    "# Shuffle Split\n",
    "Unlike KFold, ShuffleSplit leaves out a percentage of the data, not to be used in the train or validation sets. To do so we must decide what the train and test sizes are, as well as the number of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf509c9e-d50b-4007-8a16-686073f1d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = ss)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
